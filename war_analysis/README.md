# fMRI Analysis Pipeline for WAR Task

This project provides a modular, configurable, and reproducible pipeline for conducting first-level and group-level fMRI analyses for the WAR task, built primarily around AFNI. It is designed to streamline the process, allowing researchers to easily define and execute various analysis models without modifying core scripts.

---

## Table of Contents
- [Prerequisites](#prerequisites)
- [Directory Structure](#directory-structure)
- [Configuration](#configuration)
  - [Main Configuration (`analysis_configs/main_config.toml`)](#main-configuration-analysis_configsmain_configtoml)
  - [Analysis Model Configuration (`analysis_configs/analysis_models.toml`)](#analysis-model-configuration-analysis_configsanalysis_modelstoml)
- [Workflow](#workflow)
  - [First-Level Analysis (`run_analysis.py`)](#first-level-analysis-run_analysispy)
  - [Group-Level Analysis (`run_group_level.py`)](#group-level-analysis-run_group_levelpy)
- [Output Structure](#output-structure)
- [Advanced Usage](#advanced-usage)
  - [Adding a New Analysis Model](#adding-a-new-analysis-model)
  - [Running Specific Subjects](#running-specific-subjects)
- [Troubleshooting](#troubleshooting)
- [Contributing](#contributing)

---

## Prerequisites

1.  **AFNI**: The core analysis software. Ensure that AFNI commands (`afni_proc.py`, `3dttest++`, `sswarper2`, `timing_tool.py`, etc.) are available in your shell's path.
2.  **Python 3**: The pipeline's controller scripts are written in Python.
3.  **TOML Library**: The configuration files use the TOML format. Install the Python library:
    ```bash
    pip install toml
    ```
4.  **dcm2niix**: Used by `mri_file_preprocess.py` for DICOM to NIfTI conversion. Ensure it's installed and accessible.

---

## Directory Structure

The ecosystem is organized to separate logic, configuration, and results, promoting clarity and maintainability.

```
/war_analysis/
├── analysis_configs/     # All pipeline configuration files.
│   ├── main_config.toml  # Global settings for the pipeline.
│   └── analysis_models.toml # Definitions for each GLM analysis model.
├── logs/                 # Log files generated by each processing step for each subject.
├── old_scripts/          # Original, monolithic shell scripts (archived after refactoring).
├── scripts/              # Modular bash scripts, each handling a specific step of the pipeline.
│   ├── 00_create_timings.sh      # Generates AFNI .1D timing files.
│   ├── 01_preprocess_anat.sh     # Performs anatomical preprocessing (SSWarper).
│   ├── 02_preprocess_func.sh     # Performs functional preprocessing (afni_proc.py).
│   ├── 03_run_glm.sh             # Runs the GLM regression step (afni_proc.py).
│   └── 04_run_group_analysis.sh  # Template for group-level analyses.
├── utils/                # Helper Python scripts for data preparation (e.g., ERA file processing).
│   ├── create_tr_magnitude_file.py
│   ├── mri_file_preprocess.py
│   ├── process_era_files.py
│   └── rename_subjects.py
├── run_analysis.py       # Main Python controller for all FIRST-LEVEL analyses.
├── run_group_level.py    # Main Python controller for all GROUP-LEVEL analyses (to be implemented).
└── README.md             # This documentation file.
```

---

## Configuration

The entire pipeline is controlled by two main configuration files located in `analysis_configs/`. These files allow you to define subjects, paths, and analysis-specific parameters without altering any code.

### Main Configuration (`analysis_configs/main_config.toml`)

This file contains global settings that apply across the entire pipeline.

**IMPORTANT**: You must edit the `input_dir` and `output_dir` paths in this file before running the pipeline for the first time.

```toml
# Absolute path to the directory containing the raw BIDS data.
# Example: /data/raw_bids
input_dir = "/path/to/your/bids_data"

# Absolute path to the directory where all outputs (derivatives) will be saved.
# This directory will contain preprocessed data, GLM results, and group analyses.
# Example: /data/derivatives/war_analysis
output_dir = "/path/to/your/derivatives"

# --- AFNI Parameters ---
# Default parameters used across most analyses.
blur_size = 4.0
tr = 2.0
censor_motion_threshold = 0.5
censor_outlier_threshold = 0.05

# --- Subjects ---
# List of subjects to be processed.
# Each subject can have multiple sessions, each with its own specific parameters.

[[subjects]]
id = "sub-AL01"
group = "MDMA"
sessions = [
  { id = 1, lag_block_1 = 10, lag_block_2 = 12 }, # Session 1 with specific lags
  { id = 2, lag_block_1 = 9,  lag_block_2 = 11 }  # Session 2 with different lags
]

[[subjects]]
id = "sub-AL02"
group = "Control"
sessions = [
  { id = 1, lag_block_1 = 8, lag_block_2 = 8 }
]

[[subjects]]
id = "sub-AL03"
group = "MDMA"
sessions = [
  { id = 1 } # No lag specified for this session, will default to 0.
]
```

### Analysis Model Configuration (`analysis_configs/analysis_models.toml`)

This file defines the specific parameters for each first-level GLM variation. Each `[model_name]` section represents a distinct analysis. You can add, remove, or modify analyses here without changing any Python or Bash code.

**Structure of a Model Entry:**

```toml
[model_name]
description = "A brief description of what this model analyzes."
stim_files = ["path/to/stim_file1.1D", "path/to/stim_file2.1D"] # Paths relative to subject's func/timings directory.
stim_labels = ["LABEL1", "LABEL2"] # Corresponding labels for regressors.
basis = "BLOCK(duration,onset_delay)" # AFNI basis function (e.g., BLOCK, GAM, TENT).
glt = [ # Optional: General Linear Tests (contrasts) to compute.
    { sym = "LABEL1 - LABEL2", label = "contrast_name" },
    { sym = "LABEL1 + LABEL2", label = "sum_contrast" }
]
stim_types = "AM1" # Optional: AFNI stimulus type (e.g., AM1, AM2, file). Defaults to AM1 if not specified.
subjects = ["sub-AL01", "sub-AL03"] # Optional: Override `all_subjects` for this specific model.
```

**Examples:**

*   **`by_block` Model (Emotional Blocks):**
    This model analyzes the BOLD response to different emotional blocks (negative, positive, neutral) and a rest condition.

    ```toml
    [by_block]
description = "GLM analysis by emotional block (Negative, Positive, Neutral)."
stim_files = [
    "timings/negative_block.1D",
    "timings/positive_block.1D",
    "timings/neutral_block.1D",
    "timings/rest.1D"
]
stim_labels = ["neg_blck", "pos_blck", "neut_blck", "rest"]
basis = "BLOCK(22,1)" # Block duration of 22s, onset delay of 1s.
glt = [
    { sym = "neg_blck - neut_blck", label = "neg-neut_blck" },
    { sym = "neg_blck - rest", label = "neg_blck-rest" },
    { sym = "pos_blck - neut_blck", label = "pos-neut_blck" },
    { sym = "neg_blck - pos_blck", label = "neg-pos_blck" }
]
```

*   **`by_scr_bins` Model (Binned SCR Amplitude Modulation):**
    This model uses binned Skin Conductance Response (SCR) values to amplitude-modulate the BOLD response. Note the `stim_types = "file"` which is a special AFNI option for external amplitude files.

    ```toml
    [by_scr_bins]
description = "GLM analysis using binned SCR as a regressor."
stim_files = ["timings/scr_binned.1D"]
stim_labels = ["SCR"]
basis = "GAM" # Gamma variate basis function.
stim_types = "file" # Indicates that the .1D file already contains amplitude information.
glt = [] # No specific GLTs defined for this example.
```

---

## Workflow

The pipeline is executed via Python controller scripts that orchestrate the modular Bash scripts.

### First-Level Analysis (`run_analysis.py`)

The `run_analysis.py` script manages all first-level processing steps for individual subjects. It reads configurations from `analysis_configs/` and executes the appropriate scripts in `scripts/`.

**Key Steps (and their corresponding scripts):**

1.  **`create_timings` (`00_create_timings.sh`):**
    *   **Purpose:** Converts raw event `.tsv` files (generated during data acquisition) into AFNI-compatible `.1D` timing files. It also processes ERA files (from Ledalab) to create amplitude-modulated timing files (e.g., `scr_binned.1D`).
    *   **Inputs:** Event `.tsv` files, ERA files (e.g., `sub-XX_era_4s.txt`, `sub-XX_era_aggregated.txt`).
    *   **Outputs:** `.1D` timing files in `input_dir/sub-XX/ses-YY/func/timings/`.

2.  **`preprocess_anat` (`01_preprocess_anat.sh`):**
    *   **Purpose:** Performs anatomical preprocessing using `sswarper2`. This step aligns the subject's T1-weighted anatomical scan to a standard MNI template.
    *   **Inputs:** Subject's T1w NIfTI file (e.g., `input_dir/sub-XX/ses-YY/anat/sub-XX_ses-YY_T1w.nii.gz`), MNI template.
    *   **Outputs:** Warped anatomical images and transformation matrices in `output_dir/sub-XX/ses-YY/anat_warped/`.

3.  **`preprocess_func` (`02_preprocess_func.sh`):**
    *   **Purpose:** Executes the initial functional preprocessing steps using `afni_proc.py`. This includes slice timing correction, motion correction, alignment to anatomical space, spatial normalization to MNI, blurring, and scaling. This step creates a "clean" functional dataset ready for any GLM analysis.
    *   **Inputs:** Raw functional BOLD NIfTI files (e.g., `input_dir/sub-XX/ses-YY/func/sub-XX_ses-YY_task-war_run-Z_echo-E_bold.nii.gz`), preprocessed anatomical data.
    *   **Outputs:** Preprocessed functional data and quality control (QC) reports in `output_dir/sub-XX/ses-YY/func_preproc/`.

4.  **`glm` (`03_run_glm.sh`):**
    *   **Purpose:** Runs the General Linear Model (GLM) regression analysis using `afni_proc.py`'s `regress` block. It applies the specified stimulus timing, labels, basis functions, and contrasts defined in `analysis_models.toml`.
    *   **Inputs:** Preprocessed functional data from `preprocess_func`, `.1D` timing files from `create_timings`.
    *   **Outputs:** Statistical maps (e.g., `stats.sub-XX_modelname+tlrc`), masked statistical maps, and chauffeur images in `output_dir/sub-XX/ses-YY/glm/model_name/`.

**Example Usage for `run_analysis.py`:**

*   **Run a single step for one subject:**
    ```bash
    python run_analysis.py --subject sub-AL01 --step create_timings
    python run_analysis.py --subject sub-AL01 --step preprocess_anat
    python run_analysis.py --subject sub-AL01 --step preprocess_func
    ```

*   **Run the entire preprocessing pipeline for one subject:**
    ```bash
    python run_analysis.py --subject sub-AL01 --step preprocess
    ```

*   **Run a specific GLM analysis for one subject (assumes preprocessing is complete):**
    ```bash
    python run_analysis.py --subject sub-AL01 --analysis by_block --step glm
    ```

*   **Run the ENTIRE pipeline for a specific analysis (for all its relevant subjects defined in `main_config.toml` or `analysis_models.toml`):**
    ```bash
    python run_analysis.py --analysis by_block --step all
    ```

*   **Run in Parallel**: To speed up processing, use the `--n_procs` argument. The command below runs the full pipeline for the `by_block` analysis across all its subjects, using 4 cores.
    ```bash
    python run_analysis.py --analysis by_block --step all --n_procs 4
    ```

*   **Specify a different session:**
    ```bash
    python run_analysis.py --subject sub-AL01 --session 2 --step all --analysis by_block
    ```

### Group-Level Analysis (`run_analysis.py`)

Group-level analyses are integrated into the main `run_analysis.py` script via the `group_analysis` step. This allows for configurable and reproducible group comparisons using the results from the first-level GLMs.

**Key Features:**

-   **Configurable Models**: Define complex group analyses directly in `analysis_models.toml`.
-   **Multiple Analysis Types**: Supports both one-sample t-tests (`3dttest++`) and linear mixed-effects modeling (`3dLMEr`).
-   **Automatic Data Handling**: The script automatically finds subjects, filters them by group, collects the correct first-level statistical files, and generates the data tables required by `3dLMEr`.

**Configuration for Group Analysis (in `analysis_configs/analysis_models.toml`):**

Within a first-level model (e.g., `[by_block]`), you can define a list of group analyses using the `[[by_block.group_analyses]]` table format.

*   **`3dLMEr` Example (Between-Groups):**

    ```toml
    [[by_block.group_analyses]]
    name = "mdma_vs_control_s1" # A unique name for this group model
    type = "3dLMEr"
    model = "group*stimulus+(1|Subj)" # The statistical model for 3dLMEr
    description = "Compare MDMA vs Control groups in session 1."
    groups = ["MDMA", "Control"] # Subject groups from main_config.toml
    sessions = [1] # Which session(s) to include
    contrasts = ["neg_blck#0_Coef", "pos_blck#0_Coef", "neut_blck#0_Coef"] # Sub-bricks from 1st-level
    stimulus_labels = ["neg", "pos", "neut"] # Labels for the 'stimulus' factor in the LME model
    glt = [ # General linear tests for the group model
        { sym = "group : 1*MDMA -1*Control stimulus : 1*neg", label = "neg_mdma_gt_control" }
    ]
    ```

*   **`3dttest++` Example (One-Sample T-Test):**

    ```toml
    [[by_block.group_analyses]]
    name = "mdma_s1_neg_vs_neut"
    type = "3dttest++"
    description = "One-sample t-test for the MDMA group in session 1 on the neg-neut contrast."
    groups = ["MDMA"]
    sessions = [1]
    contrast = "neg-neut_blck#0_Coef" # The specific sub-brick from the first-level GLT to test.
    setA_label = "MDMA_S1_NegVsNeut" # Label for the output dataset.
    ```

**Example Usage for Group Analysis:**

To run a group analysis, you must specify the `group_analysis` step, the parent first-level analysis, and the name of the group model to run.

```bash
# Run the 3dLMEr model comparing MDMA and Control groups
python run_analysis.py \
    --step group_analysis \
    --analysis by_block \
    --group_model mdma_vs_control_s1

# Run the 3dttest++ model for the MDMA group
python run_analysis.py \
    --step group_analysis \
    --analysis by_block \
    --group_model mdma_s1_neg_vs_neut
```

---

## Output Structure

The pipeline generates a structured output directory within the `output_dir` specified in `main_config.toml`.

```
/your_output_dir/
├── sub-AL01/
│   ├── ses-1/
│   │   ├── anat_warped/      # Output from 01_preprocess_anat.sh
│   │   ├── func_preproc/     # Output from 02_preprocess_func.sh
│   │   └── glm/
│   │       ├── by_block/     # Output from 03_run_glm.sh for 'by_block' model
│   │       └── by_scr_bins/  # Output from 03_run_glm.sh for 'by_scr_bins' model
│   └── ses-2/
│       └── ...
├── sub-AL02/
│   └── ...
└── group_analysis/
    ├── by_block/             # Parent analysis for the group models
    │   ├── mdma_vs_control_s1/ # Results for this group model
    │   │   ├── result_mdma_vs_control_s1+tlrc.HEAD
    │   │   ├── result_mdma_vs_control_s1+tlrc.BRIK
    │   │   ├── group_mask+tlrc.HEAD
    │   │   └── data_table.txt
    │   └── mdma_s1_neg_vs_neut/
    │       └── ...
    └── ...
```

---

## Advanced Usage

### Adding a New Analysis Model

To add a new first-level analysis:

1.  Open `analysis_configs/analysis_models.toml`.
2.  Add a new `[model_name]` entry, defining its `description`, `stim_files`, `stim_labels`, `basis`, and optional `glt` and `stim_types`.
3.  Ensure that the `.1D` timing files specified in `stim_files` are correctly generated by `00_create_timings.sh` or are otherwise available in the subject's `func/timings/` directory. You might need to modify `00_create_timings.sh` or add a new helper script in `utils/` if new types of timing files are required.
4.  You can then run it immediately using `run_analysis.py`:
    `python run_analysis.py --analysis my_new_model --step all`

### Running Specific Subjects

Both `run_analysis.py` and `run_group_level.py` accept a `--subject` argument to process only a single subject, or you can define a `subjects` array within an `analysis_models.toml` entry to run a model on a specific subset of subjects.

*   **Override config for a single run:**
    ```bash
    python run_analysis.py --analysis by_block --step glm --subject sub-AL05
    ```
*   **Define subjects within `analysis_models.toml`:**
    ```toml
    [special_analysis]
description = "A model for a specific subset of subjects."
stim_files = ["timings/special.1D"]
stim_labels = ["SPECIAL"]
basis = "BLOCK(10,0)"
subjects = ["sub-AL01", "sub-AL03", "sub-AL07"] # This model will ONLY run for these subjects.
    ```

---

## Troubleshooting

*   **`input_dir` or `output_dir` not set:** Ensure these paths are correctly defined in `analysis_configs/main_config.toml`. They must be absolute paths.
*   **AFNI commands not found:** Verify that AFNI is installed and its binaries are in your system's PATH environment variable.
*   **TOML parsing errors:** Check `analysis_configs/*.toml` files for syntax errors (e.g., missing quotes, incorrect array/table formatting).
*   **Script not found errors:** Ensure the `scripts/` and `utils/` directories are correctly populated and that the Python controller can locate the Bash scripts.
*   **Log files:** Always check the `logs/` directory for detailed output and error messages from individual script executions.

---

## Contributing

To extend or modify the pipeline:

1.  **New Preprocessing/GLM Step:** Create a new Bash script in `scripts/` (e.g., `0X_new_step.sh`). Ensure it takes `--subject`, `--session`, `--input`, and `--output` arguments consistently. Add it to the `script_map` in `run_analysis.py` and define a new `--step` choice.
2.  **New Analysis Model:** Define it in `analysis_models.toml` as described above.
3.  **New Utility Function:** Add Python helper scripts to `utils/`.
4.  **Group Analysis:** Implement the logic within `04_run_group_analysis.sh` and integrate it with `run_group_level.py`.

Always strive for modularity and reusability. Test your changes thoroughly.